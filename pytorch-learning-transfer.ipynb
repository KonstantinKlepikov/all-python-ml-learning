{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возьмем готовый претренированный ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model = models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Фризим слои, которые мы не хотим обновлять в дообучении. Для этого мы не даем накапливаться градиентам спмощью requires_grad() для всех параметров сети (доступны через метод paraneters())\n",
    "\n",
    "Кроме того, не будем фризить блоки BatchNorm чтобьы не терять данные на нормализации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in transfer_model.named_parameters():\n",
    "    if(\"bn\" not in name):\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Заменим последний классификационный блок своим хвостом. В данном случае двумя линейными слоями, релу и дропаутом. Пайторч хранит последний блок как переменную экземпляра, поэтому ее можно переопределить.\n",
    "\n",
    "Переменная in_features позволяет захватывать несколько активаций, входящих в слой. Аналог для исходящих out_features. Они используются в случае, когда вход следующего слоя ожидает от выхода предыдущего одну размерность, а получает другую, что вызывает ошибку."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features, 500), \n",
    "                                  nn.ReLU(), \n",
    "                                  nn.Dropout(), \n",
    "                                  nn.Linear(500, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(epochs):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output, dim=1), dim=1)[1], targets)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.2f}, Validation Loss: {:.2f}, accuracy = {:.2f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_image(path):\n",
    "    try:\n",
    "        im = Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "img_transforms = transforms.Compose([\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "train_data_path = \"pytorch_learning/train/\"\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,\n",
    "                                              transform=img_transforms, \n",
    "                                              is_valid_file=check_image)\n",
    "val_data_path = \"pytorch_learning/val/\"\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,\n",
    "                                            transform=img_transforms, \n",
    "                                            is_valid_file=check_image)\n",
    "# не используем\n",
    "test_data_path = \"pytorch_learning/test/\"\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path, \n",
    "                                             transform=img_transforms, \n",
    "                                             is_valid_file=check_image)\n",
    "\n",
    "batch_size=64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, \n",
    "                                                batch_size=batch_size,\n",
    "                                                shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, \n",
    "                                               batch_size=batch_size, \n",
    "                                               shuffle=True)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\") \n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transfer_model.to(device)\n",
    "optimizer = optim.Adam(transfer_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.58, Validation Loss: 0.33, accuracy = 0.84\n",
      "Epoch: 1, Training Loss: 0.17, Validation Loss: 0.24, accuracy = 0.89\n",
      "Epoch: 2, Training Loss: 0.07, Validation Loss: 0.22, accuracy = 0.93\n",
      "Epoch: 3, Training Loss: 0.02, Validation Loss: 0.46, accuracy = 0.84\n",
      "Epoch: 4, Training Loss: 0.02, Validation Loss: 0.34, accuracy = 0.92\n",
      "Epoch: 5, Training Loss: 0.01, Validation Loss: 0.56, accuracy = 0.85\n",
      "Epoch: 6, Training Loss: 0.01, Validation Loss: 0.35, accuracy = 0.91\n",
      "Epoch: 7, Training Loss: 0.04, Validation Loss: 0.34, accuracy = 0.91\n",
      "Epoch: 8, Training Loss: 0.05, Validation Loss: 0.60, accuracy = 0.86\n",
      "Epoch: 9, Training Loss: 0.06, Validation Loss: 0.28, accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, \n",
    "      optimizer,\n",
    "      torch.nn.CrossEntropyLoss(), \n",
    "      train_data_loader,\n",
    "      val_data_loader, \n",
    "      epochs=10, \n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['cat','fish']\n",
    "def predictor(image_for_predict):\n",
    "    img = Image.open(image_for_predict) \n",
    "    img = img_transforms(img).to(device)\n",
    "    # сеть ожидает 4d тензор, т.к. используются пакеты \n",
    "    # (первое значение в тензоре - число изображений в пакете)\n",
    "    # т.к. у нас пакетов нет, создаем пакет длиной 1 с помощью unsqueeze(0)\n",
    "    # который добавляет новую размерность в начало тензора\n",
    "    img = img.unsqueeze(0)\n",
    "\n",
    "    prediction = F.softmax(transfer_model(img), dim=1)\n",
    "    prediction = prediction.argmax()\n",
    "    print(labels[prediction])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n"
     ]
    }
   ],
   "source": [
    "# cat\n",
    "predictor(\"pytorch_learning/test/cat/2041806579_e4a7f31b32.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n"
     ]
    }
   ],
   "source": [
    "# fish\n",
    "predictor(\"pytorch_learning/test/fish/1609947018_bfa1fcd6b2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n"
     ]
    }
   ],
   "source": [
    "# fish\n",
    "predictor(\"pytorch_learning/test/fish/2869804396_c4127ccec6.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Вычисление скорости обучения\n",
    "\n",
    "Метод описан [тут](https://arxiv.org/abs/1506.01186) и предполагает, что в течении эпохи, скорость обучения начинается с небольшой, увеличивается с каждым минибачем, в итоге получается высокая в конце эпохи. Задача заключается в нахождении отношения скорости к потерям и затем в выборе такого показателя скорости, при котором происходит наибольшее снижение потерь (это можно сделать например визуально по графику)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# упрощенный пример реализации метода в open.ai\n",
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if(len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    if(len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, \n",
    "                        torch.nn.CrossEntropyLoss(),\n",
    "                        optimizer, \n",
    "                        train_data_loader,\n",
    "                        device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Функция фактически обучает модель и связана с настройками оптимизатора. Поэтому необходимо предварительно сохранить и перегрузить модель, чтобы вернуться в состояние, в котором она находилась до вызова find_lr(). Кроме того придется повторно инициализировать оптимизатор.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дифференциальная скорость обучения\n",
    "\n",
    "Можно дообучить слои предобученной сети, не используя обучение слоев с нуля. Для этого в pytorch можно задать специфическую скорость обучения для этих слоев и разморозитиь их.\n",
    "\n",
    "В данном случае сделаем это с последним и предпоследним слоем перед добавленным нами классификатором"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# базируемся на вычисленной скорости обучения \n",
    "# (на самом деле сетья не перегружал и выбрал значение на шару - просто лемонстрация)\n",
    "found_lr = 1e-7\n",
    "optimizer = optim.Adam([\n",
    "    {'params': transfer_model.layer4.parameters(), 'lr': found_lr/3},\n",
    "    {'params': transfer_model.layer3.parameters(), 'lr': found_lr/9}\n",
    "], lr= found_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "unfreeze_layers = [transfer_model.layer3, transfer_model.layer4]\n",
    "for layer in unfreeze_layers:\n",
    "    for param in layer.parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Training Loss: 0.02, Validation Loss: 0.27, accuracy = 0.94\n",
      "Epoch: 1, Training Loss: 0.02, Validation Loss: 0.29, accuracy = 0.94\n",
      "Epoch: 2, Training Loss: 0.02, Validation Loss: 0.28, accuracy = 0.94\n",
      "Epoch: 3, Training Loss: 0.01, Validation Loss: 0.28, accuracy = 0.94\n",
      "Epoch: 4, Training Loss: 0.02, Validation Loss: 0.27, accuracy = 0.93\n",
      "Epoch: 5, Training Loss: 0.01, Validation Loss: 0.28, accuracy = 0.94\n",
      "Epoch: 6, Training Loss: 0.01, Validation Loss: 0.29, accuracy = 0.93\n",
      "Epoch: 7, Training Loss: 0.01, Validation Loss: 0.28, accuracy = 0.94\n",
      "Epoch: 8, Training Loss: 0.01, Validation Loss: 0.29, accuracy = 0.94\n",
      "Epoch: 9, Training Loss: 0.00, Validation Loss: 0.29, accuracy = 0.94\n"
     ]
    }
   ],
   "source": [
    "train(transfer_model, \n",
    "      optimizer,\n",
    "      torch.nn.CrossEntropyLoss(), \n",
    "      train_data_loader,\n",
    "      val_data_loader, \n",
    "      epochs=10, \n",
    "      device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Аугментация данных\n",
    "\n",
    "Методы для аугментации доступны в torchvision.\n",
    "\n",
    "Методы могут возрващать PIL-images, тензоры или конвертить тензоры в изображения и наоборот.\n",
    "\n",
    "Есть два способа - преобразования из коробки и кастомные преобразования с использованием пользовательских функций\n",
    "\n",
    "[Подробнее](https://pytorch.org/docs/stable/torchvision/transforms.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# трансформация из коробки осуществляется через стандартный папйплайн pytorch с вызовом определенных функций\n",
    "img_augment = transforms.Compose([\n",
    "    transforms.CenterCrop(10),\n",
    "    transforms.ToTensor(),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для аугментации в разных цветовых форматах у pytorch методов нет (RGB, HSV, YUV, LAB). Но есть обертка, с помощью которой можно написать свой метод для конвертера изображений.\n",
    "\n",
    "[transforms.Lambda()](https://pytorch.org/docs/stable/torchvision/transforms.html#generic-transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _random_color_apace(x):\n",
    "    output = x.convert('HSV') # метод PIL конвертирует в нужный формат\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все изображения в пакете\n",
    "color_transforms = transforms.Lambda(lambda x: _random_color_apace(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сдучайные изображения в пакете с вероятностью 0.5\n",
    "random_color_transforms = transforms.RandomApply([color_transforms])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда анонимной функции недостаточно, можно создать свой класс, который будет производить преобразования над PILL-изображениями или над тензорами. Такой класс должен определять два метода \\__call\\__ в котором будет определен конвеер и \\__repr\\__, который вернет строковое представление преобразования, а так-же состояния для тестирования. **Ограничений на методы в call нет - можно хоть в другую сеть пихать**\n",
    "\n",
    "Для примера - добавление к тензору случайного гауссового шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Noise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "    \n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        return tensor.add_(noise)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr_ = f\"{self.__class__.__name__  }(mean={self.mean},sttdev={self.stddev})\"\n",
    "        return repr_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_transform_pipeline = transforms.Compose([\n",
    "    random_color_transforms, \n",
    "    Noise(0.1, 0.05)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один метод улучшить качество обучения модели - обучиться вначале на изображениях маленького формата, затем обученную сеть еще раз прогнать на изображдениях большего формата и затем еще раз на самых больших."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_pipeline = transforms.Compose([\n",
    "    transforms.Resize(64),\n",
    "    # other augmentation\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                    std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этот метод правда замедлит обучение, так как каждый раз в пайплайне будет выполняться ресайз. Можно подготовить картинки заранее, но это требует места на диске.\n",
    "\n",
    "Кроме того, можно повышать глубину сети, к примеру двигаясь от ResNet34, к 101 и 152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Еще один способ - использовать несколько моделей, а затем построить их ансамбль. В данном случае мы прост овыберем среднее из двух резнетов. stack объединяет массив тензоров моделей. mean вычисляет среднее (в данном случае по первой размерности тензора). argmax выбирает индекс тензора с наивысшей размерностью. Естественно это самый банальный из возможных примеров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/konstantin/anaconda3/envs/pytorch-gpu/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]\n",
    "predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] \n",
    "avg_prediction = torch.stack(predictions).mean(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(260, device='cuda:0')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0006, 0.0017, 0.0011,  ..., 0.0027, 0.0005, 0.0005]],\n",
       "\n",
       "        [[0.0008, 0.0006, 0.0006,  ..., 0.0005, 0.0004, 0.0003]]],\n",
       "       device='cuda:0', grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
